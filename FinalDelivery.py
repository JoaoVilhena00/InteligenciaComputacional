# -*- coding: utf-8 -*-
"""DataProcessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LmDW9FpHbn72zBrBlVVcwQXeA9H0QUJH
"""

# pyarrow might be needed to read the data
!python -m pip install Cython
!python -m pip install -e git+https://git@github.com/alercebroker/turbo-fats#egg=turbofats
!python -m pip install -e git+https://git@github.com/alercebroker/mhps#egg=mhps
!python -m pip install -e git+https://git@github.com/alercebroker/P4J#egg=P4J
!python -m pip install pyarrow
!python -m pip install -e git+https://git@github.com/alercebroker/lc_classifier#egg=lc_classifier



"""# Data"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

from lc_classifier.utils import LightcurveBuilder

pathData =  "/content/drive/MyDrive/ColabNotebooks/InteligenciaComputacional/Projeto/Dados/"

!ls /content/drive/MyDrive/ColabNotebooks/InteligenciaComputacional/Projeto/Dados/

# read the datasets
df_crossmatches = pd.read_csv(pathData+"dfcrossmatches_prioritized_v7.0.1.csv")
df_detections = pd.read_csv(pathData+"detections_SNe_v7.0.1.csv")

"""## Join Data"""

# print the datasets
df_crossmatches.set_index('oid')
df_detections.set_index('objectId')
#df_joined = pd.concat([df_crossmatches, df_detections], axis=1, join='inner')
df_joined = pd.merge(df_crossmatches, df_detections, left_on='oid', right_on='objectId')
df_joined.pop('Unnamed: 0')
#df_joined.rename(columns = {'Unnamed: 0':'key'}, inplace = True)
print(df_joined)

"""Removing columns with data that we will not use"""

df_joined = df_joined[['oid', 'classALeRCE', 'fid', 'mjd', 'magpsf_corr', 'sigmapsf_corr']]

print("Removed useless columns")

print(df_joined)

"""Removed band with less samples"""

#drop rows that contain specific 'value' in 'column_name'
df_oneBand = df_joined[df_joined.fid == 2]
df_oneBand = df_joined

print(df_oneBand)

"""Removed rows with NaN on magnitude and error since these do not provide aditional value."""

df_oneBand_noNaN = df_oneBand.dropna(subset=["magpsf_corr","sigmapsf_corr"])
df_oneBand_noNaN.rename(columns = {'fid':'band','mjd':'time','magpsf_corr':'magnitude','sigmapsf_corr':'error'}, inplace = True)
print(df_oneBand_noNaN)
df_oneBand_noNaN.set_index('oid')

"""Removed rows with error equal to 100 because that data isnt relevant

"""

df_oneBand_noNaN = df_oneBand_noNaN[df_oneBand_noNaN.error != 100.000000]
df_oneBand_noNaN

"""## Feature Extractor"""

bands = [1,2]

#n_curves = len(pd.unique(df_oneBand_noNaN['oid']))
#n_curves = len(df_oneBand_noNaN)
df_grouped = df_oneBand_noNaN.groupby('oid')

lightcurves = []
# iterate over each group
for group_name, df_group in df_grouped:

    lightcurve_builder = LightcurveBuilder(f'{group_name}')
    band = []
    #band = bands[0]
    time = []
    magnitude = []
    error = []
    # iterate over each lightcurve with same id and fill vectors
    for row_index, row in df_group.iterrows():
      for current_band in bands: # TODO not done correctly for bands
        band.append(row['band'])
        time.append(row['time'])
        magnitude.append(row['magnitude'])
        error.append(row['error'])

    # builds the lightcurve with vectors
    lightcurve_builder.add_band(
        band,
        time,
        magnitude,
        error)

    lightcurve = lightcurve_builder.build_dataframe()
    lightcurves.append(lightcurve)
lightcurves = pd.concat(lightcurves)
lightcurves

cnt = 0
for row_index, curve in lightcurves.iterrows():
  if int(curve['band']) == 1:
    cnt+=1
print('Number of band 1: '+str(cnt))

"""Plotting the light curve"""

fig = plt.figure(figsize=(10, 12))
fig.set_facecolor('white')
bands = [2]
plt.subplot(2, 1, 1)
for band in bands:
    obs_in_band = lightcurves[lightcurves.band == band]
    plt.errorbar(
        obs_in_band.time,
        obs_in_band.magnitude,
        yerr=obs_in_band.error,
        fmt='.',
        label=band
    )

plt.xlabel('Time [days]')
plt.ylabel('Magnitude')
plt.title('Synthetic light curve')
plt.legend()
plt.gca().invert_yaxis()
'''
plt.subplot(2, 1, 2)
for band in bands:
    obs_in_band = lightcurves[lightcurves.band == band]
    plt.errorbar(
        (obs_in_band.time % true_period) / true_period * 2 * np.pi,
        obs_in_band.magnitude,
        yerr=obs_in_band.error,
        fmt='.',
        label=band
    )

plt.xlabel('Phase [radians]')
plt.ylabel('Magnitude')
plt.title(f'Folded light curve (Period {true_period:.3f} days)')
plt.legend()
plt.gca().invert_yaxis()

plt.show()
'''

from lc_classifier.features import MHPSExtractor, PeriodExtractor, GPDRWExtractor
from lc_classifier.features import FoldedKimExtractor
from lc_classifier.features import HarmonicsExtractor, IQRExtractor
from lc_classifier.features import PowerRateExtractor
from lc_classifier.features import TurboFatsFeatureExtractor

from lc_classifier.features import FeatureExtractorComposer

bands = [1,2]

feature_extractor = FeatureExtractorComposer(
    [
        MHPSExtractor(bands),
        PeriodExtractor(bands),
        GPDRWExtractor(bands),
        FoldedKimExtractor(bands),
        HarmonicsExtractor(bands),
        IQRExtractor(bands),
        PowerRateExtractor(bands),
        TurboFatsFeatureExtractor(bands)
    ]
)

features = feature_extractor.compute_features(lightcurves)
features

features_cpy=features
df_noDup=df_oneBand_noNaN.drop_duplicates(subset=['oid'])

features_class=pd.merge(features,df_noDup[['oid','classALeRCE']],left_on='oid',right_on='oid',how='right')
print(features_class.head())

features_class.to_csv(pathData+'features_raw_both_bands.csv', encoding='utf-8', index=True)

"""## Read Features Raw CSV"""

import time
t1 = time.perf_counter()

features_raw = pd.read_csv(pathData + 'features_raw.csv')
features_raw.pop('Unnamed: 0')
features_raw

"""## Visualize and Analyse Data

### Class Imbalance
"""

pip install -U matplotlib

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

classes = features_raw.classALeRCE.values

x = classes

plt.hist(x, density=True, bins=12)  # density=False would make counts
plt.ylabel('Probability')
plt.xlabel('Data');

"""- As we can see data is quite **unbalanced**, so we should aim to balance it further ahead.

### Missing Data
"""

nan_data = features_raw.isna().sum()
print(nan_data)

indexes = []
for i in range(0,len(nan_data)):
  if nan_data[i]==max(nan_data):
      indexes.append(i)

y = nan_data
x = list(features_raw.columns)

print("\nFeatures with max NaNs:")
for i in indexes:
  print(x[i])

fig = plt.figure(figsize = (10, 5))
 
# creating the bar plot
plt.bar(x, y, color ='maroon',
        width = 0.4)
 
plt.xlabel("Features")
plt.ylabel("No. of NaNs")
plt.title("NaNs by feature")
plt.show()

"""From this plot we can clearly see that there are two atributes named *Eta_e_2* and *MaxSlope_2* with almost all values as *NaN*, as such these have no real value and will be removed further down the line"""

rows_w_nan = features_raw.isnull().any(axis=1).sum()

percentage_rows_w_nan = rows_w_nan/len(features_raw)*100

#print('Percentage of Rows with atleast one NaN: ' + str(percentage_rows_w_nan) + '%')

row_num_of_nans = features_raw.isnull().sum(axis=1)
#print('\n\n')

x = classes

plt.hist(row_num_of_nans, density=False, bins=10, range=(0,12))  # density=False would make counts
plt.title('Distribution of number of NaNs in each lightcurve')
plt.ylabel('Number of lightcurves')
plt.xlabel('Number of NaNs');

print('Same plot but with diferent scale...\n')

plt.hist(row_num_of_nans, density=False, bins=10)  # density=False would make counts
plt.title('Distribution of number of NaNs in each lightcurve')
plt.ylabel('Number of lightcurves')
plt.xlabel('Number of NaNs');

"""As such we can see that a lot of NaNs are present so we cannot simply eliminate rows with NaNs otherwise we would only have left a very small portion of our data.

So maybe only eliminating those with a lot of NaNs makes more sense, and keeping those with less NaNs, for example deleting the lightcurves with more than 40 NaNs being that there is a total of 62 atributes.

## Preprocess Data

### Class Imbalance

We will be joining classes *SNII*, *SNIIb*, *SNIIn* and treating them as a new class *SNII_Joined*, this way our classes will be at least a little more balanced.
"""

# Commented out IPython magic to ensure Python compatibility.
# Applying the condition
features_raw.loc[features_raw["classALeRCE"] == "SNII", "classALeRCE"] = "SNII_Joined"
features_raw.loc[features_raw["classALeRCE"] == "SNIIb", "classALeRCE"] = "SNII_Joined"
features_raw.loc[features_raw["classALeRCE"] == "SNIIn", "classALeRCE"] = "SNII_Joined"


# New plot of class imbalance
# %matplotlib inline

classes = features_raw.classALeRCE.values

x = classes

plt.hist(x, density=True, bins=12)  # density=False would make counts
plt.title('Class Unbalance after Joining some classes')
plt.ylabel('Probability')
plt.xlabel('Data');

"""### Remove some NaNs

As previously discussed, there are two atributes named *Eta_e_2* and *MaxSlope_2* with almost all values as *NaN*, as such these have no real value and will be removed.
"""

features_pop = features_raw.copy()

#features_pop.pop('Eta_e_1')
#features_pop.pop('MaxSlope_1')
features_pop.pop('MaxSlope_2')
features_pop.pop('Eta_e_2')

features_pop

"""Now we can see that there are no more atribute outliers in terms of NaNs."""

nan_data = features_pop.isna().sum()
#print(nan_data)
y = nan_data
x = list(features_pop.columns)
fig = plt.figure(figsize = (10, 5))
 
# creating the bar plot
plt.bar(x, y, color ='maroon',
        width = 0.4)
 
plt.xlabel("Features")
plt.ylabel("No. of NaNs")
plt.title("NaNs by feature")
plt.show()

"""Now we will remove the lightcurves with more than 60% of the atributes as NaNs since these do not provide much real value"""

features_no_big_nan = features_pop[features_pop.isnull().sum(axis=1) < 40]

features_no_big_nan

rows_w_nan = features_no_big_nan.isnull().any(axis=1).sum()

percentage_rows_w_nan = rows_w_nan/len(features_no_big_nan)*100

#print('Percentage of Rows with atleast one NaN: ' + str(percentage_rows_w_nan) + '%')

row_num_of_nans = features_no_big_nan.isnull().sum(axis=1)
#print('\n\n')

plt.hist(row_num_of_nans, density=False, bins=10)  # density=False would make counts
plt.title('Distribution of number of NaNs in each lightcurve')
plt.ylabel('Number of lightcurves')
plt.xlabel('Number of NaNs');

nan_data = features_no_big_nan.isna().sum()
print(nan_data)
y = nan_data
x = list(features_no_big_nan.columns)
fig = plt.figure(figsize = (10, 5))
 
# creating the bar plot
plt.bar(x, y, color ='maroon',
        width = 0.4)
 
plt.xlabel("Features")
plt.ylabel("No. of NaNs")
plt.title("NaNs by feature")
plt.show()

"""Now we can see that only a few atributes with NaNs are left and even these have few NaNs, the most has around 50 which is still a relatively acceptable number.

So now we must fill these NaN values, there are several possible methods but we choose to use the average of that column in order to fill these NaNs, so as to not have much impact in the results.

### Normalize
"""

#normalized_features=(features_no_big_nan-features_no_big_nan.mean())/features_no_big_nan.std()
#normalized_features

normalized_only_features = features_no_big_nan.select_dtypes(include='number')

normalized_only_features = (normalized_only_features - normalized_only_features.mean()) / (normalized_only_features.max() - normalized_only_features.min())


features_no_big_nan[normalized_only_features.columns] = normalized_only_features

normalized_features = features_no_big_nan.copy()

normalized_features

nan_data = normalized_features.isna().sum()
#print(nan_data)
y = nan_data
x = list(normalized_features.columns)
fig = plt.figure(figsize = (10, 5))
 
# creating the bar plot
plt.bar(x, y, color ='maroon',
        width = 0.4)
 
plt.xlabel("Features")
plt.ylabel("No. of NaNs")
plt.title("NaNs by feature")
plt.show()

"""### Fill Remaining NaNs"""

features_nan_filled = normalized_features.fillna(normalized_features.mean())

features_nan_filled

nan_data = features_nan_filled.isna().sum()
#print(nan_data)
y = nan_data
x = list(features_nan_filled.columns)
fig = plt.figure(figsize = (10, 5))
 
# creating the bar plot
plt.bar(x, y, color ='maroon',
        width = 0.4)
 
plt.xlabel("Features")
plt.ylabel("No. of NaNs")
plt.title("NaNs by feature")
plt.show()

"""As we can see no more NaNs exist and as such we are ready to store this fully preprocessed dataset into a csv to then be used by classification models."""

features_nan_filled.to_csv(pathData+'features_preprocessed.csv', encoding='utf-8', index=True)

"""# Read Features Preprocessed CSV"""

features_preprocessed = pd.read_csv(pathData + 'features_preprocessed.csv')
features_preprocessed.pop('Unnamed: 0')
features_preprocessed
features_preprocessed_cpy = features_preprocessed

"""# Classification"""

from sklearn.model_selection import train_test_split

classes = features_preprocessed["classALeRCE"].to_frame()
#converting the classes to numbers
mapping = {'SLSN': 0, 'SNII_Joined': 1, 'SNIa': 2,'SNIbc':3}
classes = classes.replace({'classALeRCE': mapping})

classes = classes.to_numpy()
classes = classes.flatten()
print(classes)
#removing the class from the original dataframe
features_preprocessed_cpy = features_preprocessed_cpy.drop(['classALeRCE'], axis=1)
features_preprocessed_cpy = features_preprocessed_cpy.drop(['oid'], axis=1)
print(features_preprocessed_cpy)

"""##Train Test split and Oversampling"""

X_train, X_test, y_train, y_test = train_test_split(features_preprocessed_cpy,classes,test_size=0.3,random_state=109)

!python -m pip install imbalanced-learn

"""Visualizing umber of samples of each class before oversampling"""

classes = []
classes = np.array(classes)
mapping = {'SLSN': 0, 'SNII_Joined': 1, 'SNIa': 2,'SNIbc':3}

for i in range(0,len(y_train)):
  if y_train[i] == 0:
    classes=np.append(classes,'SLSN')
  if y_train[i] == 1:
    classes=np.append(classes,'SNII_Joined')
  if y_train[i] == 2:
    classes=np.append(classes,'SNIa')
  if y_train[i] == 3:
    classes=np.append(classes,'SNIbc')

print(classes)
    


plt.hist(classes, density=True, bins=12)  # density=False would make counts
plt.ylabel('Probability')
plt.xlabel('Data');

from imblearn.over_sampling import RandomOverSampler
from imblearn.over_sampling import SMOTE 

oversample = RandomOverSampler(sampling_strategy='all')

#X_over, y_over = oversample.fit_resample(X_train, y_train)

sm = SMOTE(random_state=42)
X_over, y_over = sm.fit_resample(X_train, y_train)

print(X_over)
print(X_train)

classes = []
classes = np.array(classes)
mapping = {'SLSN': 0, 'SNII_Joined': 1, 'SNIa': 2,'SNIbc':3}

for i in range(0,len(y_over)):
  if y_over[i] == 0:
    classes=np.append(classes,'SLSN')
  if y_over[i] == 1:
    classes=np.append(classes,'SNII_Joined')
  if y_over[i] == 2:
    classes=np.append(classes,'SNIa')
  if y_over[i] == 3:
    classes=np.append(classes,'SNIbc')

print(classes)
    


plt.hist(classes, density=True, bins=12)  # density=False would make counts
plt.ylabel('Probability')
plt.xlabel('Data');

"""## Feature Selection with Random Forest"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix, ConfusionMatrixDisplay

clf = RandomForestClassifier(
    n_estimators=1000,
    criterion="gini",
    max_depth=3,
    max_features="sqrt",
    n_jobs=-1,
    class_weight="balanced"
)

#clf.fit(X_train, y_train)
clf.fit(X_over, y_over)

y_pred = clf.predict(X_test)
cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)

def plot_confusion_matrix(
    y_test,
    y_pred,
    classes,
    normalize=True,
    ax=None,
):
    if ax is None:
        fig, ax = plt.subplots(figsize=(8, 6), dpi=90)
        fig.tight_layout()

    ConfusionMatrixDisplay.from_predictions(
        y_test,
        y_pred,
        cmap=plt.cm.Blues,
        normalize="true" if normalize else None,
        ax=ax
    )
    
    title = f"Matriz de confusion{' no' if not normalize else ''} normalizada"
    
    ax.set_title(title)
    # ax.colorbar()
    tick_marks = np.arange(len(classes))
    ax.set_xticks(tick_marks, classes)
    ax.set_yticks(tick_marks, classes)

    ax.set_ylabel("True label")
    ax.set_xlabel("Predicted label")
    ax.grid("off")

def mean_recall(cm):
    cm = cm.astype("float") / cm.sum(axis=1)[:, np.newaxis]
    return cm.diagonal().mean()

target_names = [
    "SLSN",
    "SNII_Joined",
    "SNIa",
    "SNIbc"
]

plot_confusion_matrix(y_test, y_pred, target_names, normalize=True)
plot_confusion_matrix(y_test, y_pred, target_names, normalize=False)

print(f"\nPromedio recall por clase (validacion) {mean_recall(cm):.3f}")

y_train_pred = clf.predict(X_train)
train_cm = confusion_matrix(y_train, y_train_pred)

print(f"Promedio recall por clase (training) {mean_recall(train_cm):.3f}")

print("Numero de ejemplos en validacion clasificados correctamente: %d" % accuracy_score(y_test, y_pred, normalize=False))
print("\nCaracteristicas ordenadas por importancia (RF)")
feature_importances = clf.feature_importances_
importance_order = np.argsort(-feature_importances)
feature_names = [
"MHPS_ratio_2",
"MHPS_low_2",
"MHPS_high_2",
"MHPS_non_zero_2",
"MHPS_PN_flag_2",
"Multiband_period",
"PPE",
"Period_band_2",
"delta_period_2",
"GP_DRW_sigma_2", 
"GP_DRW_tau_2",
"Psi_CS_2",
"Psi_eta_2",
"Harmonics_mag_1_2",
"Harmonics_mag_2_2",
"Harmonics_mag_3_2",
"Harmonics_mag_4_2",
"Harmonics_mag_5_2",
"Harmonics_mag_6_2",
"Harmonics_mag_7_2",
"Harmonics_phase_2_2",
"Harmonics_phase_3_2",
"Harmonics_phase_4_2",
"Harmonics_phase_5_2",
"Harmonics_phase_6_2",
"Harmonics_phase_7_2",
"Harmonics_mse_2",
"iqr_2",
"Power_rate_1/4",
"Power_rate_1/3",
"Power_rate_1/2",
"Power_rate_2",
"Power_rate_3",
"Power_rate_4",
"Amplitude_2",
"AndersonDarling_2",
"Autocor_length_2",
"Beyond1Std_2",
"Con_2",
"Gskew_2",
"Mean_2",
"Meanvariance_2",
"MedianAbsDev_2",
"MedianBRP_2",
"PairSlopeTrend_2",
"PercentAmplitude_2",
"Q31_2",
"Rcs_2",
"Skew_2",
"SmallKurtosis_2",
"Std_2",
"StetsonK_2",
"Pvar_2",
"ExcessVar_2",
"SF_ML_amplitude_2",
"SF_ML_gamma_2",
"IAR_phi_2",
"LinearTrend_2"]
for index in importance_order:
    print(f"\t{feature_importances[index]:.3f} {feature_names[index]}")

"""Now we will keep only 80% importance features"""

total_importance = 0.6
importance = 0
important_features = np.empty(0)
important_features_names = []


for index in importance_order:
    #print(f"\t{feature_importances[index]:.3f} {feature_names[index]}")
    importance += feature_importances[index]
    important_features = np.append(important_features,int(index))
    important_features_names = np.append(important_features_names,feature_names[index])
    if importance >= total_importance:
      break
important_features = important_features.astype(int)

print('Features with a total sum of ' + str(total_importance) + ' importance:')
for index in important_features:
  print(f"\t{feature_importances[index]:.3f} {feature_names[index]}")
print('\nTotal number of features: ' + str(len(important_features)))

feature_to_keep = []
for index in important_features:
  feature_to_keep.append(feature_names[index])

X_train = X_train.filter(feature_to_keep)
X_over = X_over.filter(feature_to_keep)

X_test = X_test.filter(feature_to_keep)
print(X_train)

"""## SVM"""

#Import svm model
from sklearn import svm

#Create a svm Classifier
clf = svm.SVC(C=1, kernel='rbf', gamma=0.5, probability=True) # Gaussian Kernel

#Train the model using the training sets
clf.fit(X_over, y_over)

#Predict the response for test dataset
y_pred = clf.predict(X_test)

"""##Model Evaluation"""

#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics

# Model Accuracy: how often is the classifier correct?
print("Balanced Accuracy:",metrics.balanced_accuracy_score(y_test, y_pred))

# Model Precision: what percentage of positive tuples are labeled as such?
print("Balanced Precision:",metrics.precision_score(y_test, y_pred, average='macro'))

# Model Recall: what percentage of positive tuples are labelled as such?
print("Balanced Recall:",metrics.recall_score(y_test, y_pred, average='macro'))

"""Confusion Matrix"""

# Plot non-normalized confusion matrix
from sklearn.metrics import ConfusionMatrixDisplay

titles_options = [
    ("Confusion matrix, without normalization", None),
    ("Normalized confusion matrix", "true"),
]
for title, normalize in titles_options:
    disp = ConfusionMatrixDisplay.from_estimator(
        clf,
        X_test,
        y_test,
        display_labels= ["SLSN","SNII_Joined","SNIa","SNIbc"],
        cmap=plt.cm.Blues,
        normalize=normalize,
    )
    disp.ax_.set_title(title)

    print(title)
    print(disp.confusion_matrix)

plt.show()

"""## Random Forest"""

clf = RandomForestClassifier(
    n_estimators=1000,
    criterion="gini",
    max_depth=3,
    max_features="sqrt",
    n_jobs=-1,
    class_weight= "balanced",
    
)

clf.fit(X_over, y_over)

y_pred = clf.predict(X_test)
cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)

def plot_confusion_matrix(
    y_test,
    y_pred,
    classes,
    normalize=True,
    ax=None,
  
):
    if ax is None:
        fig, ax = plt.subplots(figsize=(8, 6), dpi=90)
        fig.tight_layout()

    ConfusionMatrixDisplay.from_predictions(
        y_test,
        y_pred,
        cmap=plt.cm.Blues,
        normalize="true" if normalize else None,
        ax=ax
    )
    
    title = f"Matriz de confusion{' no' if not normalize else ''} normalizada"
    
    ax.set_title(title)
    # ax.colorbar()
    tick_marks = np.arange(len(classes))
    ax.set_xticks(tick_marks, classes)
    ax.set_yticks(tick_marks, classes)

    ax.set_ylabel("True label")
    ax.set_xlabel("Predicted label")
    ax.grid("off")

def mean_recall(cm):
    cm = cm.astype("float") / cm.sum(axis=1)[:, np.newaxis]
    return cm.diagonal().mean()

target_names = [
    "SLSN",
    "SNII_Joined",
    "SNIa",
    "SNIbc"
]

plot_confusion_matrix(y_test, y_pred, target_names, normalize=True)
plot_confusion_matrix(y_test, y_pred, target_names, normalize=False)

print(f"\nPromedio recall por clase (validacion) {mean_recall(cm):.3f}")

y_train_pred = clf.predict(X_train)
train_cm = confusion_matrix(y_train, y_train_pred)

print(f"Promedio recall por clase (training) {mean_recall(train_cm):.3f}")

print("Numero de ejemplos en validacion clasificados correctamente: %d" % accuracy_score(y_test, y_pred, normalize=False))
print("\nCaracteristicas ordenadas por importancia (RF)")
feature_importances = clf.feature_importances_
importance_order = np.argsort(-feature_importances)
feature_names = important_features_names
#feature_names = feature_names
for index in importance_order:
    print(f"\t{feature_importances[index]:.3f} {feature_names[index]}")

#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics

# Model Accuracy: how often is the classifier correct?
print("Balanced Accuracy:",metrics.balanced_accuracy_score(y_test, y_pred))

# Model Precision: what percentage of positive tuples are labeled as such?
print("Balanced Precision:",metrics.precision_score(y_test, y_pred, average='macro'))

# Model Recall: what percentage of positive tuples are labelled as such?
print("Balanced Recall:",metrics.recall_score(y_test, y_pred, average='macro'))

"""## K-NN"""

#Import knearest neighbors Classifier model
from sklearn.neighbors import KNeighborsClassifier

#Create KNN Classifier
clf = KNeighborsClassifier(n_neighbors=3)

#Train the model using the training sets
clf.fit(X_train, y_train)

#Predict the response for test dataset
y_pred = clf.predict(X_test)

y_pred = clf.predict(X_test)
cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)

def plot_confusion_matrix(
    y_test,
    y_pred,
    classes,
    normalize=True,
    ax=None,
):
    if ax is None:
        fig, ax = plt.subplots(figsize=(8, 6), dpi=90)
        fig.tight_layout()

    ConfusionMatrixDisplay.from_predictions(
        y_test,
        y_pred,
        cmap=plt.cm.Blues,
        normalize="true" if normalize else None,
        ax=ax
    )
    
    title = f"Matriz de confusion{' no' if not normalize else ''} normalizada"
    
    ax.set_title(title)
    # ax.colorbar()
    tick_marks = np.arange(len(classes))
    ax.set_xticks(tick_marks, classes)
    ax.set_yticks(tick_marks, classes)

    ax.set_ylabel("True label")
    ax.set_xlabel("Predicted label")
    ax.grid("off")

def mean_recall(cm):
    cm = cm.astype("float") / cm.sum(axis=1)[:, np.newaxis]
    return cm.diagonal().mean()

target_names = [
    "SLSN",
    "SNII_Joined",
    "SNIa",
    "SNIbc"
]

plot_confusion_matrix(y_test, y_pred, target_names, normalize=True)
plot_confusion_matrix(y_test, y_pred, target_names, normalize=False)

print(f"\nPromedio recall por clase (validacion) {mean_recall(cm):.3f}")

y_train_pred = clf.predict(X_train)
train_cm = confusion_matrix(y_train, y_train_pred)

print(f"Promedio recall por clase (training) {mean_recall(train_cm):.3f}")

print("Numero de ejemplos en validacion clasificados correctamente: %d" % accuracy_score(y_test, y_pred, normalize=False))

#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics

# Model Accuracy: how often is the classifier correct?
print("Balanced Accuracy:",metrics.balanced_accuracy_score(y_test, y_pred))

# Model Precision: what percentage of positive tuples are labeled as such?
print("Balanced Precision:",metrics.precision_score(y_test, y_pred, average='macro'))

# Model Recall: what percentage of positive tuples are labelled as such?
print("Balanced Recall:",metrics.recall_score(y_test, y_pred, average='macro'))

"""Counting the time needed to run (without taking into account the extraction of features)"""

t2 = time.perf_counter()
print('time taken to run:',t2-t1)