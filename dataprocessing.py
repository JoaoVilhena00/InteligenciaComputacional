# -*- coding: utf-8 -*-
"""DataProcessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LmDW9FpHbn72zBrBlVVcwQXeA9H0QUJH
"""

# pyarrow might be needed to read the data
!python -m pip install Cython
!python -m pip install -e git+https://git@github.com/alercebroker/turbo-fats#egg=turbofats
!python -m pip install -e git+https://git@github.com/alercebroker/mhps#egg=mhps
!python -m pip install -e git+https://git@github.com/alercebroker/P4J#egg=P4J
!python -m pip install pyarrow
!python -m pip install -e git+https://git@github.com/alercebroker/lc_classifier#egg=lc_classifier

"""# Data"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

from lc_classifier.utils import LightcurveBuilder

pathData =  "/content/drive/MyDrive/ColabNotebooks/InteligenciaComputacional/Projeto/Dados/"

!ls /content/drive/MyDrive/ColabNotebooks/InteligenciaComputacional/Projeto/Dados/

# read the datasets
df_crossmatches = pd.read_csv(pathData+"dfcrossmatches_prioritized_v7.0.1.csv")
df_detections = pd.read_csv(pathData+"detections_SNe_v7.0.1.csv")

"""## Join Data"""

# print the datasets
df_crossmatches.set_index('oid')
df_detections.set_index('objectId')
#df_joined = pd.concat([df_crossmatches, df_detections], axis=1, join='inner')
df_joined = pd.merge(df_crossmatches, df_detections, left_on='oid', right_on='objectId')
df_joined.pop('Unnamed: 0')
#df_joined.rename(columns = {'Unnamed: 0':'key'}, inplace = True)
print(df_joined)

"""Removing columns with data that we will not use"""

df_joined = df_joined[['oid', 'classALeRCE', 'fid', 'mjd', 'magpsf_corr', 'sigmapsf_corr']]

print("Removed useless columns")

print(df_joined)

"""Removed band with less samples"""

#drop rows that contain specific 'value' in 'column_name'
df_oneBand = df_joined[df_joined.fid == 2]
print(df_oneBand)

"""Removed rows with NaN on magnitude and error since these do not provide aditional value."""

df_oneBand_noNaN = df_oneBand.dropna(subset=["magpsf_corr","sigmapsf_corr"])
df_oneBand_noNaN.rename(columns = {'fid':'band','mjd':'time','magpsf_corr':'magnitude','sigmapsf_corr':'error'}, inplace = True)
print(df_oneBand_noNaN)
df_oneBand_noNaN.set_index('oid')

"""Removed rows with error equal to 100 because that data isnt relevant

"""

df_oneBand_noNaN = df_oneBand_noNaN[df_oneBand_noNaN.error != 100.000000]
df_oneBand_noNaN

"""## Feature Extractor"""

bands = [2]

#n_curves = len(pd.unique(df_oneBand_noNaN['oid']))
#n_curves = len(df_oneBand_noNaN)
df_grouped = df_oneBand_noNaN.groupby('oid')

lightcurves = []
# iterate over each group
for group_name, df_group in df_grouped:

    lightcurve_builder = LightcurveBuilder(f'{group_name}')
    #band = []
    band = bands[0]
    time = []
    magnitude = []
    error = []
    # iterate over each lightcurve with same id and fill vectors
    for row_index, row in df_group.iterrows():
      #for band in bands: # TODO not done correctly for bands
        #band.append(row['band'])
        time.append(row['time'])
        magnitude.append(row['magnitude'])
        error.append(row['error'])

    # builds the lightcurve with vectors
    lightcurve_builder.add_band(
        band,
        time,
        magnitude,
        error)

    lightcurve = lightcurve_builder.build_dataframe()
    lightcurves.append(lightcurve)
lightcurves = pd.concat(lightcurves)
lightcurves

"""Plotting the light curve"""

fig = plt.figure(figsize=(10, 12))
fig.set_facecolor('white')
bands = [2]
plt.subplot(2, 1, 1)
for band in bands:
    obs_in_band = lightcurves[lightcurves.band == band]
    plt.errorbar(
        obs_in_band.time,
        obs_in_band.magnitude,
        yerr=obs_in_band.error,
        fmt='.',
        label=band
    )

plt.xlabel('Time [days]')
plt.ylabel('Magnitude')
plt.title('Synthetic light curve')
plt.legend()
plt.gca().invert_yaxis()
'''
plt.subplot(2, 1, 2)
for band in bands:
    obs_in_band = lightcurves[lightcurves.band == band]
    plt.errorbar(
        (obs_in_band.time % true_period) / true_period * 2 * np.pi,
        obs_in_band.magnitude,
        yerr=obs_in_band.error,
        fmt='.',
        label=band
    )

plt.xlabel('Phase [radians]')
plt.ylabel('Magnitude')
plt.title(f'Folded light curve (Period {true_period:.3f} days)')
plt.legend()
plt.gca().invert_yaxis()

plt.show()
'''

from lc_classifier.features import MHPSExtractor, PeriodExtractor, GPDRWExtractor
from lc_classifier.features import FoldedKimExtractor
from lc_classifier.features import HarmonicsExtractor, IQRExtractor
from lc_classifier.features import PowerRateExtractor
from lc_classifier.features import TurboFatsFeatureExtractor

from lc_classifier.features import FeatureExtractorComposer

bands = [2]

feature_extractor = FeatureExtractorComposer(
    [
        MHPSExtractor(bands),
        PeriodExtractor(bands),
        GPDRWExtractor(bands),
        FoldedKimExtractor(bands),
        HarmonicsExtractor(bands),
        IQRExtractor(bands),
        PowerRateExtractor(bands),
        TurboFatsFeatureExtractor(bands)
    ]
)

features = feature_extractor.compute_features(lightcurves)
features

features_cpy=features
df_noDup=df_oneBand_noNaN.drop_duplicates(subset=['oid'])

features_class=pd.merge(features,df_noDup[['oid','classALeRCE']],left_on='oid',right_on='oid',how='right')
print(features_class.head())

features_class.to_csv(pathData+'features_raw.csv', encoding='utf-8', index=True)

"""## Read Features Raw CSV"""

features_raw = pd.read_csv(pathData + 'features_raw.csv')
features_raw.pop('Unnamed: 0')
features_raw

"""## Visualize and Analyse Data

### Class Imbalance
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

classes = features_raw.classALeRCE.values

x = classes

plt.hist(x, density=True, bins=12)  # density=False would make counts
plt.ylabel('Probability')
plt.xlabel('Data');

"""- As we can see data is quite **unbalanced**, so we should aim to balance it further ahead.

### Missing Data
"""

nan_data = features_raw.isna().sum()
print(nan_data)
y = nan_data
x = list(features_raw.columns)
fig = plt.figure(figsize = (10, 5))
 
# creating the bar plot
plt.bar(x, y, color ='maroon',
        width = 0.4)
 
plt.xlabel("Features")
plt.ylabel("No. of NaNs")
plt.title("NaNs by feature")
plt.show()

"""From this plot we can clearly see that there are two atributes named *Eta_e_2* and *MaxSlope_2* with almost all values as *NaN*, as such these have no real value and will be removed further down the line"""

rows_w_nan = features_raw.isnull().any(axis=1).sum()

percentage_rows_w_nan = rows_w_nan/len(features_raw)*100

#print('Percentage of Rows with atleast one NaN: ' + str(percentage_rows_w_nan) + '%')

row_num_of_nans = features_raw.isnull().sum(axis=1)
#print('\n\n')

x = classes

plt.hist(row_num_of_nans, density=False, bins=10, range=(0,12))  # density=False would make counts
plt.title('Distribution of number of NaNs in each lightcurve')
plt.ylabel('Number of lightcurves')
plt.xlabel('Number of NaNs');

print('Same plot but with diferent scale...\n')

plt.hist(row_num_of_nans, density=False, bins=10)  # density=False would make counts
plt.title('Distribution of number of NaNs in each lightcurve')
plt.ylabel('Number of lightcurves')
plt.xlabel('Number of NaNs');

"""As such we can see that a lot of NaNs are present so we cannot simply eliminate rows with NaNs otherwise we would only have left a very small portion of our data.

So maybe only eliminating those with a lot of NaNs makes more sense, and keeping those with less NaNs, for example deleting the lightcurves with more than 40 NaNs being that there is a total of 62 atributes.

## Preprocess Data

### Class Imbalance

We will be joining classes *SNII*, *SNIIb*, *SNIIn* and treating them as a new class *SNII_Joined*, this way our classes will be at least a little more balanced.
"""

# Commented out IPython magic to ensure Python compatibility.
# Applying the condition
features_raw.loc[features_raw["classALeRCE"] == "SNII", "classALeRCE"] = "SNII_Joined"
features_raw.loc[features_raw["classALeRCE"] == "SNIIb", "classALeRCE"] = "SNII_Joined"
features_raw.loc[features_raw["classALeRCE"] == "SNIIn", "classALeRCE"] = "SNII_Joined"


# New plot of class imbalance
# %matplotlib inline

classes = features_raw.classALeRCE.values

x = classes

plt.hist(x, density=True, bins=12)  # density=False would make counts
plt.title('Class Unbalance after Joining some classes')
plt.ylabel('Probability')
plt.xlabel('Data');

"""### Remove some NaNs

As previously discussed, there are two atributes named *Eta_e_2* and *MaxSlope_2* with almost all values as *NaN*, as such these have no real value and will be removed.
"""

features_pop = features_raw.copy()

features_pop.pop('Eta_e_2')
features_pop.pop('MaxSlope_2')

features_pop

"""Now we can see that there are no more atribute outliers in terms of NaNs."""

nan_data = features_pop.isna().sum()
#print(nan_data)
y = nan_data
x = list(features_pop.columns)
fig = plt.figure(figsize = (10, 5))
 
# creating the bar plot
plt.bar(x, y, color ='maroon',
        width = 0.4)
 
plt.xlabel("Features")
plt.ylabel("No. of NaNs")
plt.title("NaNs by feature")
plt.show()

"""Now we will remove the lightcurves with more than 60% of the atributes as NaNs since these do not provide much real value"""

features_no_big_nan = features_pop[features_pop.isnull().sum(axis=1) < 40]

features_no_big_nan

rows_w_nan = features_no_big_nan.isnull().any(axis=1).sum()

percentage_rows_w_nan = rows_w_nan/len(features_no_big_nan)*100

#print('Percentage of Rows with atleast one NaN: ' + str(percentage_rows_w_nan) + '%')

row_num_of_nans = features_no_big_nan.isnull().sum(axis=1)
#print('\n\n')

plt.hist(row_num_of_nans, density=False, bins=10)  # density=False would make counts
plt.title('Distribution of number of NaNs in each lightcurve')
plt.ylabel('Number of lightcurves')
plt.xlabel('Number of NaNs');

nan_data = features_no_big_nan.isna().sum()
print(nan_data)
y = nan_data
x = list(features_no_big_nan.columns)
fig = plt.figure(figsize = (10, 5))
 
# creating the bar plot
plt.bar(x, y, color ='maroon',
        width = 0.4)
 
plt.xlabel("Features")
plt.ylabel("No. of NaNs")
plt.title("NaNs by feature")
plt.show()

"""Now we can see that only a few atributes with NaNs are left and even these have few NaNs, the most has around 35 which is still a relatively acceptable number.

So now we must fill these NaN values, there are several possible methods but we choose to use the average of that column in order to fill these NaNs, so as to not have much impact in the results.

### Normalize
"""

#normalized_features=(features_no_big_nan-features_no_big_nan.mean())/features_no_big_nan.std()
#normalized_features

normalized_only_features = features_no_big_nan.select_dtypes(include='number')

normalized_only_features = (normalized_only_features - normalized_only_features.mean()) / (normalized_only_features.max() - normalized_only_features.min())


features_no_big_nan[normalized_only_features.columns] = normalized_only_features

normalized_features = features_no_big_nan.copy()

normalized_features

nan_data = normalized_features.isna().sum()
#print(nan_data)
y = nan_data
x = list(normalized_features.columns)
fig = plt.figure(figsize = (10, 5))
 
# creating the bar plot
plt.bar(x, y, color ='maroon',
        width = 0.4)
 
plt.xlabel("Features")
plt.ylabel("No. of NaNs")
plt.title("NaNs by feature")
plt.show()

"""### Fill Remaining NaNs"""

features_nan_filled = normalized_features.fillna(normalized_features.mean())

features_nan_filled

nan_data = features_nan_filled.isna().sum()
#print(nan_data)
y = nan_data
x = list(features_nan_filled.columns)
fig = plt.figure(figsize = (10, 5))
 
# creating the bar plot
plt.bar(x, y, color ='maroon',
        width = 0.4)
 
plt.xlabel("Features")
plt.ylabel("No. of NaNs")
plt.title("NaNs by feature")
plt.show()

"""As we can see no more NaNs exist and as such we are ready to store this fully preprocessed dataset into a csv to then be used by classification models."""

features_nan_filled.to_csv(pathData+'features_preprocessed.csv', encoding='utf-8', index=True)

"""## Read Features Preprocessed CSV"""

features_preprocessed = pd.read_csv(pathData + 'features_preprocessed.csv')
features_preprocessed.pop('Unnamed: 0')
features_preprocessed
features_preprocessed_cpy = features_preprocessed

"""# Classification"""

from sklearn.model_selection import train_test_split

classes = features_preprocessed["classALeRCE"].to_frame()
#converting the classes to numbers
mapping = {'SLSN': 0, 'SNII_Joined': 1, 'SNIa': 2,'SNIbc':3}
classes = classes.replace({'classALeRCE': mapping})

classes = classes.to_numpy()
classes = classes.flatten()
print(classes)
#removing the class from the original dataframe
features_preprocessed_cpy = features_preprocessed_cpy.drop(['classALeRCE'], axis=1)
features_preprocessed_cpy = features_preprocessed_cpy.drop(['oid'], axis=1)
print(features_preprocessed_cpy)

"""##Train Test split and Oversampling"""

X_train, X_test, y_train, y_test = train_test_split(features_preprocessed_cpy,classes,test_size=0.3,random_state=109)

!python -m pip install imbalanced-learn

from imblearn.over_sampling import RandomOverSampler
oversample = RandomOverSampler(sampling_strategy='minority')

X_over, y_over = oversample.fit_resample(X_train, y_train)
print(X_over)
print(X_train)

"""## SVM"""

#Import svm model
from sklearn import svm

#Create a svm Classifier
clf = svm.SVC(C=0.2,kernel='linear') # Linear Kernel

#Train the model using the training sets
clf.fit(X_over, y_over)

#Predict the response for test dataset
y_pred = clf.predict(X_test)

"""### Model Evaluation"""

#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics

# Model Accuracy: how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

# Model Precision: what percentage of positive tuples are labeled as such?
print("Precision:",metrics.precision_score(y_test, y_pred, average='weighted'))

# Model Recall: what percentage of positive tuples are labelled as such?
print("Recall:",metrics.recall_score(y_test, y_pred, average='weighted'))

"""Confusion Matrix"""

# Plot non-normalized confusion matrix
from sklearn.metrics import ConfusionMatrixDisplay

titles_options = [
    ("Confusion matrix, without normalization", None),
    ("Normalized confusion matrix", "true"),
]
for title, normalize in titles_options:
    disp = ConfusionMatrixDisplay.from_estimator(
        clf,
        X_test,
        y_test,
        display_labels= [0,1,2,3],
        cmap=plt.cm.Blues,
        normalize=normalize,
    )
    disp.ax_.set_title(title)

    print(title)
    print(disp.confusion_matrix)

plt.show()